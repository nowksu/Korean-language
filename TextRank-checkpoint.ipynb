{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\anaconda\\lib\\site-packages (from newspaper3k) (5.1.2)\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\anaconda\\lib\\site-packages (from newspaper3k) (2.8.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\anaconda\\lib\\site-packages (from newspaper3k) (4.8.0)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\anaconda\\lib\\site-packages (from newspaper3k) (6.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\anaconda\\lib\\site-packages (from newspaper3k) (4.4.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\anaconda\\lib\\site-packages (from newspaper3k) (3.4.5)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\anaconda\\lib\\site-packages (from newspaper3k) (2.22.0)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/0e/9ab599d6e78f0340bb1d1e28ddeacb38c8bb7f91a1b0eae9a24e9603782f/tldextract-2.2.2-py2.py3-none-any.whl (48kB)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\anaconda\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.12.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (1.9.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\anaconda\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (41.4.0)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: jieba3k, tinysegmenter, feedparser, feedfinder2\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp37-none-any.whl size=7398414 sha256=f9e3c229d568f3b5b1d76c3448b403f4d2537c39c4bf8e704129e38d3f9ab825\n",
      "  Stored in directory: C:\\Users\\이경수\\AppData\\Local\\pip\\Cache\\wheels\\83\\15\\9c\\a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp37-none-any.whl size=13542 sha256=366a67bed0e7afb112261d4cdb4e8a4096b558b117520e322d4bc9060a912e38\n",
      "  Stored in directory: C:\\Users\\이경수\\AppData\\Local\\pip\\Cache\\wheels\\81\\2b\\43\\a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
      "  Building wheel for feedparser (setup.py): started\n",
      "  Building wheel for feedparser (setup.py): finished with status 'done'\n",
      "  Created wheel for feedparser: filename=feedparser-5.2.1-cp37-none-any.whl size=44944 sha256=8d61798f7db9e8c7acc0f4ab95b16679d36022c38832a16fc0e04b23bbd5aeec\n",
      "  Stored in directory: C:\\Users\\이경수\\AppData\\Local\\pip\\Cache\\wheels\\8c\\69\\b7\\f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp37-none-any.whl size=3362 sha256=20acd3b34895ed09c42ee42d8900abdfaa91e8e6d83e98bfcacb59f40de6ba7a\n",
      "  Stored in directory: C:\\Users\\이경수\\AppData\\Local\\pip\\Cache\\wheels\\de\\03\\ca\\778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
      "Successfully built jieba3k tinysegmenter feedparser feedfinder2\n",
      "Installing collected packages: jieba3k, tinysegmenter, feedparser, feedfinder2, requests-file, tldextract, cssselect, newspaper3k\n",
      "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 tinysegmenter-0.3 tldextract-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\anaconda\\lib\\site-packages (from konlpy) (1.16.5)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from konlpy) (0.4.1)\n",
      "Collecting tweepy>=3.7.0 (from konlpy)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
      "Collecting beautifulsoup4==4.6.0 (from konlpy)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\anaconda\\lib\\site-packages (from konlpy) (4.4.1)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/86/579cc52801a8d40570c2a9ec813d2920e8be740be15055e5583098c04278/JPype1-1.0.1-cp37-cp37m-win_amd64.whl (1.6MB)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\anaconda\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (2.22.0)\n",
      "Collecting requests-oauthlib>=0.7.0 (from tweepy>=3.7.0->konlpy)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\anaconda\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
      "Collecting typing-extensions; python_version < \"3.8\" (from JPype1>=0.7.0->konlpy)\n",
      "  Downloading https://files.pythonhosted.org/packages/0c/0e/3f026d0645d699e7320b59952146d56ad7c374e9cd72cd16e7c74e657a0f/typing_extensions-3.7.4.2-py3-none-any.whl\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy, beautifulsoup4, typing-extensions, JPype1, konlpy\n",
      "  Found existing installation: beautifulsoup4 4.8.0\n",
      "    Uninstalling beautifulsoup4-4.8.0:\n",
      "      Successfully uninstalled beautifulsoup4-4.8.0\n",
      "Successfully installed JPype1-1.0.1 beautifulsoup4-4.6.0 konlpy-0.5.2 oauthlib-3.1.0 requests-oauthlib-1.3.0 tweepy-3.9.0 typing-extensions-3.7.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[속보] 안산 유치원 '집단 식중독' 유증상자 1명 추가…총 44명\n",
      "내년 4월 보궐선거에서의 더불어민주당 서울·부산시장 후보 공천 여부에 관심이 쏠리고 있습니다. 고(故) 박원순 전 서울시장은 성추행 혐의로 피소된 뒤 극단적 선택을 했고, 오거돈 전 부산시장은 강제추행 혐의를 인정하고 사퇴해 공석이 됐습니다. 민주당은 당헌에 ‘당 소속\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "#크롤링할 url 주소 입력\n",
    "url = 'http://v.media.daum.net/v/20170604205121164'\n",
    "url = 'https://www.hankyung.com/society/article/2020062616417'\n",
    "#언어가 한국어이므로 language='ko'로 설정\n",
    "a = Article(url, language='ko')\n",
    "a.download()\n",
    "a.parse()\n",
    "#기사 제목 가져오기\n",
    "print(a.title)\n",
    "#기사 내용 가져오기(150자)\n",
    "\n",
    "print(a.text[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내년 4월 보궐선거에서의 더불어민주당 서울·부산시장 후보 공천 여부에 관심이 쏠리고 있습니다. 고(故) 박원순 전 서울시장은 성추행 혐의로 피소된 뒤 극단적 선택을 했고, 오거돈 전 부산시장은 강제추행 혐의를 인정하고 사퇴해 공석이 됐습니다. 민주당은 당헌에 ‘당 소속 선출직 공직자가 중대 잘못으로 직위를 상실해 재·보궐선거를 실시할 경우 후보자를 추천하지 않는다’고 규정, 당 안팎에서 무공천이 바람직하다는 여론이 일고 있습니다. 하지만 역대급 규모 재보선으로 치러지는 데다 대선 전초전 성격까지 겹쳐 민주당은 무공천 여부에 대한 정확한 입장을 내놓지 않고 있습니다. 시민들 의견은 갈리고 있습니다. 책임정치 구현이란 측면에서 민주당이 후보를 내는 것은 적절치 않다는 지적이 나옵니다. 반면 후보를 내고 유권자들이 판단하도록 하는 게 공당의 의무라는 의견도 적지 않습니다. 민주당, 서울·부산시장 후보를 공천해야 한다고 보십니까?\n"
     ]
    }
   ],
   "source": [
    "print(a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jvm 관련 에러 발생 시, konlpy와 Jpype1의 버젼이 충돌나서 생기는 문제! 다음 버전 지정으로 쉽게 해결이 가능하다!\n",
    "# !pip install jpype1==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [텍스트 크롤링, 문장 단위 분리, 명사 추출] 과정을 SentenceTokenizer 클래스로 만들었다.\n",
    "\n",
    "class SentenceTokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.kkma = Kkma()\n",
    "        self.okt = Okt()\n",
    "        self.stopwords = ['중인', '만큼', '마찬가지', '꼬집었', \"연합뉴스\", \"데일리\", \"동아일보\", \"중앙일보\", \"조선일보\", \"기자\"\n",
    "            , \"아\", \"휴\", \"아이구\", \"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"의\", \"가\", ]\n",
    "    \n",
    "    def url2sentences(self, url):\n",
    "        article = Article(url, language='ko')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        sentences = self.kkma.sentences(article.text)\n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx - 1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "\n",
    "    def text2sentences(self, text):\n",
    "        sentences = self.kkma.sentences(text)\n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx - 1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "\n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence is not '':\n",
    "                nouns.append(' '.join([noun for noun in self.kkma.nouns(str(sentence))\n",
    "                #nouns.append(' '.join([noun for noun in self.okt.nouns(str(sentence))\n",
    "                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "        return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TF-IDF 모델, 그래프 생성] 과정을 GraphMatrix 클래스로 구현했다.\n",
    "\n",
    "class GraphMatrix(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "\n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "\n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "            \n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank(object):\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        if text[:5] in ('http:', 'https'):\n",
    "            self.sentences = self.sent_tokenize.url2sentences(text)\n",
    "        else:\n",
    "            self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "\n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        return summary\n",
    "\n",
    "    def keywords(self, word_num=10):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank(url):\n",
    "    output = ''\n",
    "    textrank = TextRank(url)\n",
    "    print(textrank)\n",
    "    for row in textrank.summarize(3):\n",
    "        if len(output) == 0:\n",
    "            output += row\n",
    "        else:\n",
    "            output += \" \"+row\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.TextRank object at 0x00000252861C2988>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'넥스트 아이는 2020년 3월 말 기준으로 이 노메트리 지분 40.62%를 보유한 최대주주다. 인수 지분규모는 이 노메트리 최대주주인 넥스트 아이 지분 40.62% 와 김 준 보 이 노메트리 대표이사의 지분 17.21% 등이다. 인수 지분규모는 이 노메트리 최대주주인 넥스트 아이 지분 40.62% 와 김 준 보 이 노메트리 대표이사의 지분 17.21% 등이다.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kkma 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK가 이 노메트리 인수를 추진하고 있다.\n",
      "\n",
      "이 노 메트리는 엑스선 (X 선) 을 활용한 2 차 전지 검사장비를 만든다.\n",
      "\n",
      "넥스트 아이는 2020년 3월 말 기준으로 이 노메트리 지분 40.62%를 보유한 최대주주다.\n",
      "\n",
      "금융투자업계에 따르면 SK는 최근 이 노메트리 인수를 위한 본계약을 체결한 것으로 전해 졌다.\n",
      "\n",
      "인수 지분규모는 이 노메트리 최대주주인 넥스트 아이 지분 40.62% 와 김 준 보 이 노메트리 대표이사의 지분 17.21% 등이다.\n",
      "\n",
      "SK의 이 노메트리 인수는 2 차 전지 검사 공정부문을 강화하기 위한 것이라는 시선이 나온다.\n",
      "\n",
      "노 메트리는 엑스선 (X 선) 을 활용한 2 차 전지 검사장비를 만든다.\n",
      "\n",
      "넥스트 아이는 2020년 3월 말 기준으로 이 노메트리 지분 40.62%를 보유한 최대주주다.\n",
      "\n",
      "인수 지분규모는 이 노메트리 최대주주인 넥스트 아이 지분 40.62% 와 김 준 보 이 노메트리 대표이사의 지분 17.21% 등이다.\n",
      "\n",
      "의 이 노메트리 인수는 2 차 전지 검사 공정부문을 강화하기 위한 것이라는 시선이 나온다.\n",
      "\n",
      "keywords : ['메트리', '아이', '넥스트', '주가', '상승', '인수', '장비', '기준', '검사', '대표이사']\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "url = \"http://www.businesspost.co.kr/BP?command=article_view&num=189455\"\n",
    "textrank = TextRank(url)\n",
    "for row in textrank.summarize(10):\n",
    "    print(row)\n",
    "    print()\n",
    "print('keywords :',textrank.keywords())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'상무부는 29일( 현지시간) 윌버 로스 장관 명의의 성명을 통해 중국의 홍 콩 국가보안법 처리와 관련해 이같이 밝혔다.\\n로스 장관은 \" 홍 콩 보안법 제정으로 인해 민감한 미국의 기술이 중국 인민 해방군이나 국가안전 보위 부로 전용될 위험이 커졌다 \"며 \" 이런 위협으로 인해 미국이 홍 콩에 대한 특별 지위를 철회하게 됐다\" 고 성명에서 말했다.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank(\"https://www.hankyung.com/international/article/2020063031827\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
